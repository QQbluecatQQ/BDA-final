{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ac46d9b",
   "metadata": {},
   "source": [
    "# Generate Pair Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5deb7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairplots saved: public_pairplot.png, private_pairplot.png\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 讀取資料\n",
    "public_df = pd.read_csv(\"public_data.csv\")\n",
    "private_df = pd.read_csv(\"private_data.csv\")\n",
    "\n",
    "# 移除 id 欄位（如果有）\n",
    "public_data = public_df.drop(columns=['id'], errors='ignore')\n",
    "private_data = private_df.drop(columns=['id'], errors='ignore')\n",
    "\n",
    "# public data pairplot\n",
    "sns.pairplot(public_data, plot_kws={\"s\": 1})\n",
    "plt.suptitle(\"Pairwise Feature Plots - Public Dataset\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"public_pairplot.png\")\n",
    "plt.close()\n",
    "\n",
    "# private data pairplot\n",
    "sns.pairplot(private_data, plot_kws={\"s\": 1})\n",
    "plt.suptitle(\"Pairwise Feature Plots - Private Dataset\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"private_pairplot.png\")\n",
    "plt.close()\n",
    "\n",
    "print(\"Pairplots saved: public_pairplot.png, private_pairplot.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ffce98",
   "metadata": {},
   "source": [
    "# Generate result visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c9e5161",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheng_yu/miniconda3/envs/bda_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/cheng_yu/miniconda3/envs/bda_env/lib/python3.10/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/cheng_yu/miniconda3/envs/bda_env/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "/home/cheng_yu/miniconda3/envs/bda_env/lib/python3.10/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/cheng_yu/miniconda3/envs/bda_env/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import umap.umap_ as umap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def visualize(dataset_name, data_file, label_file, prefix):\n",
    "    # 讀取資料與 label\n",
    "    df = pd.read_csv(data_file)\n",
    "    labels = pd.read_csv(label_file)['label']\n",
    "\n",
    "    # 特徵處理\n",
    "    X = df.drop(columns=['id'])\n",
    "    X = X.loc[:, X.std() != 0].dropna(axis=1)\n",
    "    X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='tab10', s=10, alpha=0.8)\n",
    "    plt.title(f'{dataset_name} - PCA')\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    plt.savefig(f'{prefix}_pca.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # UMAP\n",
    "    reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "    X_umap = reducer.fit_transform(X_scaled)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X_umap[:, 0], X_umap[:, 1], c=labels, cmap='tab10', s=10, alpha=0.8)\n",
    "    plt.title(f'{dataset_name} - UMAP')\n",
    "    plt.xlabel('UMAP1')\n",
    "    plt.ylabel('UMAP2')\n",
    "    plt.savefig(f'{prefix}_umap.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# 設定檔名\n",
    "visualize(\"PUBLIC\", \"public_data.csv\", \"public_submission.csv\", \"public\")\n",
    "visualize(\"PRIVATE\", \"private_data.csv\", \"private_submission.csv\", \"private\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e26b6b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Public Dataset Clusters: 15\n",
      "Private Dataset Clusters: 23\n",
      "Public Dataset Clustering Evaluation:\n",
      "  Silhouette Score: 0.6101\n",
      "  Calinski-Harabasz Index: 37758.7347\n",
      "  Davies-Bouldin Index: 0.7810\n",
      "\n",
      "Private Dataset Clustering Evaluation:\n",
      "  Silhouette Score: 0.5249\n",
      "  Calinski-Harabasz Index: 114074.8280\n",
      "  Davies-Bouldin Index: 0.8592\n",
      "\n",
      "Clustering completed. Files saved:\n",
      "- public_submission.csv\n",
      "- private_submission.csv\n",
      "- public_pairplot.png\n",
      "- public_kmeans.png\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import zscore\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "IQR_THRESHOLD = 12  # IQR threshold for outlier detection\n",
    "zscore_threshold = 5  # Z-score threshold for outlier detection\n",
    "\n",
    "# ========== Preprocessing Function ==========\n",
    "def preprocess(df, feature_cols, method='iqr'):\n",
    "    df = df.copy()\n",
    "    df = df.dropna()\n",
    "    X = df[feature_cols]\n",
    "\n",
    "    # 移除常數欄位\n",
    "    non_constant_cols = X.loc[:, X.std() != 0].columns.tolist()\n",
    "    X = df[non_constant_cols]\n",
    "\n",
    "    # 移除離群值\n",
    "    if method == 'iqr':\n",
    "        Q1 = X.quantile(0.25)\n",
    "        Q3 = X.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        mask = ~((X < (Q1 - IQR_THRESHOLD * IQR)) | (X > (Q3 + IQR_THRESHOLD * IQR))).any(axis=1)\n",
    "    elif method == 'zscore':\n",
    "        z_scores = np.abs(zscore(X))\n",
    "        mask = (z_scores < zscore_threshold).all(axis=1)\n",
    "    else:\n",
    "        mask = pd.Series([True] * len(X))\n",
    "\n",
    "    df_clean = df[mask]\n",
    "    X_clean = X[mask]  # 注意：這裡要用 log 後的 X\n",
    "\n",
    "    # 標準化\n",
    "    scaler = StandardScaler()\n",
    "    X_clean_scaled = scaler.fit_transform(X_clean)\n",
    "\n",
    "    return X_clean_scaled, df_clean, non_constant_cols, scaler\n",
    "\n",
    "# ========== Clustering ==========\n",
    "def run_kmeans(X, n_clusters):\n",
    "    model = KMeans(n_clusters=n_clusters, init=\"random\", random_state=42)\n",
    "    model.fit(X)\n",
    "    return model\n",
    "\n",
    "def evaluate_clustering(X, labels):\n",
    "    sil_score = silhouette_score(X, labels)\n",
    "    ch_score = calinski_harabasz_score(X, labels)\n",
    "    db_score = davies_bouldin_score(X, labels)\n",
    "    return sil_score, ch_score, db_score\n",
    "\n",
    "def visualize_clusters(X, labels, method, file_name):\n",
    "    # PCA 降到 2 維\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='tab20', s=1)\n",
    "    plt.xlabel(\"PCA 1\")\n",
    "    plt.ylabel(\"PCA 2\")\n",
    "    plt.title(f\"{method} Clustering\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(file_name)\n",
    "    plt.close()\n",
    "\n",
    "# ========== Load Data ==========\n",
    "public_df = pd.read_csv(\"public_data.csv\")\n",
    "private_df = pd.read_csv(\"private_data.csv\")\n",
    "\n",
    "public_features = ['1', '2', '3', '4']\n",
    "private_features = ['1', '2', '3', '4', '5', '6']\n",
    "\n",
    "# ========== Preprocess ==========\n",
    "X_pub_clean, df_pub_clean, pub_cols, scaler_pub = preprocess(public_df, public_features, method='iqr')\n",
    "X_priv_clean, df_priv_clean, priv_cols, scaler_priv = preprocess(private_df, private_features, method='iqr')\n",
    "\n",
    "# ========== Fit KMeans ==========\n",
    "n_pub_clusters = 4 * len(pub_cols) - 1\n",
    "n_priv_clusters = 4 * len(priv_cols) - 1\n",
    "\n",
    "print(f\"Public Dataset Clusters: {n_pub_clusters}\")\n",
    "print(f\"Private Dataset Clusters: {n_priv_clusters}\")\n",
    "\n",
    "kmeans_pub = run_kmeans(X_pub_clean, n_pub_clusters)\n",
    "kmeans_priv = run_kmeans(X_priv_clean, n_priv_clusters)\n",
    "\n",
    "# 計算聚類指標\n",
    "sil_pub, ch_pub, db_pub = evaluate_clustering(X_pub_clean, kmeans_pub.labels_)\n",
    "sil_priv, ch_priv, db_priv = evaluate_clustering(X_priv_clean, kmeans_priv.labels_)\n",
    "\n",
    "print(f\"Public Dataset Clustering Evaluation:\")\n",
    "print(f\"  Silhouette Score: {sil_pub:.4f}\")\n",
    "print(f\"  Calinski-Harabasz Index: {ch_pub:.4f}\")\n",
    "print(f\"  Davies-Bouldin Index: {db_pub:.4f}\\n\")\n",
    "\n",
    "print(f\"Private Dataset Clustering Evaluation:\")\n",
    "print(f\"  Silhouette Score: {sil_priv:.4f}\")\n",
    "print(f\"  Calinski-Harabasz Index: {ch_priv:.4f}\")\n",
    "print(f\"  Davies-Bouldin Index: {db_priv:.4f}\\n\")\n",
    "\n",
    "# ========== Predict on ALL data ==========\n",
    "X_pub_all = scaler_pub.transform(public_df[pub_cols])\n",
    "X_priv_all = scaler_priv.transform(private_df[priv_cols])\n",
    "\n",
    "labels_pub_all = kmeans_pub.predict(X_pub_all)\n",
    "labels_priv_all = kmeans_priv.predict(X_priv_all)\n",
    "\n",
    "public_df['label'] = labels_pub_all\n",
    "private_df['label'] = labels_priv_all\n",
    "\n",
    "# ========== Save ==========\n",
    "public_df[['id', 'label']].to_csv(\"public_submission.csv\", index=False)\n",
    "private_df[['id', 'label']].to_csv(\"private_submission.csv\", index=False)\n",
    "\n",
    "# ========== Visualization ==========\n",
    "sns.pairplot(df_pub_clean.drop(columns='id', errors='ignore'), plot_kws={\"s\": 1})\n",
    "plt.suptitle(\"Pairwise Feature Plots - Public Dataset\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"public_pairplot.png\")\n",
    "plt.close()\n",
    "\n",
    "visualize_clusters(X_pub_all, labels_pub_all, \"KMeans - Public\", \"public_kmeans.png\")\n",
    "\n",
    "print(\"Clustering completed. Files saved:\")\n",
    "print(\"- public_submission.csv\")\n",
    "print(\"- private_submission.csv\")\n",
    "print(\"- public_pairplot.png\")\n",
    "print(\"- public_kmeans.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7caf64f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "577b051a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering complete.\n",
      "- public_submission.csv\n",
      "- private_submission.csv\n",
      "- public_kmeans.png\n",
      "- private_kmeans.png\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# ========== Clustering ==========\n",
    "def run_kmeans(X, n_clusters):\n",
    "    model = KMeans(n_clusters=n_clusters, init=\"random\", random_state=42)\n",
    "    model.fit(X)\n",
    "    return model\n",
    "\n",
    "def visualize_clusters(X, labels, title, file_name):\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='tab20', s=1)\n",
    "    plt.xlabel(\"Feature 2 (scaled)\")\n",
    "    plt.ylabel(\"Feature 3 (scaled)\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(file_name)\n",
    "    plt.close()\n",
    "\n",
    "# ========== Load Data ==========\n",
    "public_df = pd.read_csv(\"public_data.csv\")\n",
    "private_df = pd.read_csv(\"private_data.csv\")\n",
    "\n",
    "selected_features = ['2', '3']\n",
    "\n",
    "# ========== 預處理 ==========\n",
    "def preprocess(df, selected_features, scale_factor=2):\n",
    "    df = df.dropna(subset=selected_features)\n",
    "    X = df[selected_features].values\n",
    "    X[:, 0] *= scale_factor  # 加權 feature 2\n",
    "    X[:, 1] *= scale_factor  # 加權 feature 3\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return X_scaled, scaler\n",
    "\n",
    "# Public dataset\n",
    "X_pub, scaler_pub = preprocess(public_df, selected_features, scale_factor=2)\n",
    "kmeans_pub = run_kmeans(X_pub, n_clusters=15)\n",
    "labels_pub = kmeans_pub.predict(X_pub)\n",
    "public_df['label'] = labels_pub\n",
    "public_df[['id', 'label']].to_csv(\"public_submission.csv\", index=False)\n",
    "visualize_clusters(X_pub, labels_pub, \"KMeans on Scaled Features 2 & 3 - Public\", \"public_kmeans.png\")\n",
    "\n",
    "# Private dataset\n",
    "X_priv, _ = preprocess(private_df, selected_features, scale_factor=2)\n",
    "kmeans_priv = run_kmeans(X_priv, n_clusters=23)\n",
    "labels_priv = kmeans_priv.predict(X_priv)\n",
    "private_df['label'] = labels_priv\n",
    "private_df[['id', 'label']].to_csv(\"private_submission.csv\", index=False)\n",
    "visualize_clusters(X_priv, labels_priv, \"KMeans on Scaled Features 2 & 3 - Private\", \"private_kmeans.png\")\n",
    "\n",
    "print(\"Clustering complete.\")\n",
    "print(\"- public_submission.csv\\n- private_submission.csv\\n- public_kmeans.png\\n- private_kmeans.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f5b459c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheng_yu/miniconda3/envs/bda_env/lib/python3.10/site-packages/sklearn/manifold/_spectral_embedding.py:329: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectral clustering done using Feature 2 & 3.\n",
      "- public_submission.csv\n",
      "- public_spectral.png\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "# ========== Clustering (Spectral) ========== \n",
    "def run_spectral(X, n_clusters):\n",
    "    model = SpectralClustering(n_clusters=n_clusters, \n",
    "                                affinity='nearest_neighbors', \n",
    "                                n_neighbors=15, # 可調參\n",
    "                                assign_labels='kmeans', \n",
    "                                random_state=42)\n",
    "    labels = model.fit_predict(X)\n",
    "    return labels\n",
    "\n",
    "def visualize_clusters(X, labels, title, file_name):\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='tab20', s=1)\n",
    "    plt.xlabel(\"Feature 2\")\n",
    "    plt.ylabel(\"Feature 3\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(file_name)\n",
    "    plt.close()\n",
    "\n",
    "# ========== Load Data ==========\n",
    "public_df = pd.read_csv(\"public_data.csv\")\n",
    "private_df = pd.read_csv(\"private_data.csv\")\n",
    "\n",
    "# 選擇第 2, 3 維特徵\n",
    "selected_features = ['2', '3']\n",
    "\n",
    "# ========== 預處理 ========== \n",
    "def preprocess_simple(df, selected_features):\n",
    "    df = df.dropna(subset=selected_features)\n",
    "    X = df[selected_features]\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return X_scaled, scaler\n",
    "\n",
    "X_pub, scaler_pub = preprocess_simple(public_df, selected_features)\n",
    "\n",
    "# ========== Spectral Clustering ==========\n",
    "k_pub = 15  # 依照規定 public dataset 應該分為 15 群\n",
    "\n",
    "labels_pub = run_spectral(X_pub, k_pub)\n",
    "public_df['label'] = labels_pub\n",
    "\n",
    "# ========== Save ========== \n",
    "public_df[['id', 'label']].to_csv(\"public_submission.csv\", index=False)\n",
    "\n",
    "# ========== Visualization ========== \n",
    "visualize_clusters(X_pub, labels_pub, \"Spectral Clustering on Features 2 & 3 - Public\", \"public_spectral.png\")\n",
    "\n",
    "print(\"Spectral clustering done using Feature 2 & 3.\")\n",
    "print(\"- public_submission.csv\")\n",
    "print(\"- public_spectral.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
